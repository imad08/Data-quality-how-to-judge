# -*- coding: utf-8 -*-
"""credit_card_main .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/169n-F0A0XjloWwL0X8Tg7ki6yGGPHS6C
"""

#importing libraries
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.metrics import silhouette_score

from google.colab import drive
drive.mount('/content/gdrive')

df.drop('Time', inplace= True, axis=1)

df.drop('Class', inplace= True, axis=1)

array= df['Class']

# Standardize data
scaler = StandardScaler() 
scaled_data = scaler.fit_transform(df)
  
# Normalizing the Data 
normalized_data = normalize(scaled_data) 
  
# Converting the numpy array into a pandas DataFrame 
normalized_data = pd.DataFrame(normalized_data) 
  
# Reducing the dimensions of the data 
#pca = PCA(n_components = 2) 
#X_principal = pca.fit_transform(normalized_df) 
#X_principal = pd.DataFrame(X_principal) 
#X_principal.columns = ['P1', 'P2'] 
  
#X_principal.head(2)

normalized_data['Class']= array

normalized_data

df  = pd.read_csv("/content/gdrive/MyDrive/creditcard.csv")

df



import numpy as np
from numpy.linalg import norm


class Kmeans:
  

    def __init__(self, n_clusters, max_iter=100, random_state=123):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.random_state = random_state

    def initializ_centroids(self, X):
        np.random.RandomState(self.random_state)
        random_idx = np.random.permutation(X.shape[0])
        centroids = X[random_idx[:self.n_clusters]]
        return centroids

    def compute_centroids(self, X, labels):
        centroids = np.zeros((self.n_clusters, X.shape[1]))
        for k in range(self.n_clusters):
            centroids[k, :] = np.mean(X[labels == k, :], axis=0)
        return centroids

    def compute_distance(self, X, centroids):
        distance = np.zeros((X.shape[0], self.n_clusters))
        for k in range(self.n_clusters):
            row_norm = norm(X - centroids[k, :], axis=1)
            distance[:, k] = np.square(row_norm)
        return distance

    def find_closest_cluster(self, distance):
        return np.argmin(distance, axis=1)

    def compute_sse(self, X, labels, centroids):
        distance = np.zeros(X.shape[0])
        for k in range(self.n_clusters):
            distance[labels == k] = norm(X[labels == k] - centroids[k], axis=1)
        return np.sum(np.square(distance))
    
    def fit(self, X):
        self.centroids = self.initializ_centroids(X)
        for i in range(self.max_iter):
            old_centroids = self.centroids
            distance = self.compute_distance(X, old_centroids)
            self.labels = self.find_closest_cluster(distance)
            self.centroids = self.compute_centroids(X, self.labels)
            if np.all(old_centroids == self.centroids):
                break
        self.error = self.compute_sse(X, self.labels, self.centroids)
    
    def predict(self, X):
        distance = self.compute_distance(X, centroids)
        return self.find_closest_cluster(distance)
    
    def ret_old_centre(self,X):
      return old_centroids

import pandas as pd
from sklearn import preprocessing

x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df = pd.DataFrame(x_scaled)

df

import matplotlib.pyplot as plt
from matplotlib.image import imread
import pandas as pd
import seaborn as sns
from sklearn.datasets.samples_generator import (make_blobs,
                                                make_circles,
                                                make_moons)
#from sklearn.cluster import KMeans, SpectralClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_samples, silhouette_score
X_std = StandardScaler().fit_transform(normalized_data)

df_predicted = km.predict(df)

arr = np.array(df_predicted)

import math
 
def dotproduct(v1, v2):
  return sum((a*b) for a, b in zip(v1, v2))
 
def length(v):
  return math.sqrt(dotproduct(v, v))
 
def angle(v1, v2):
  return math.acos(dotproduct(v1, v2) / (length(v1) * length(v2)))

list_centroids=[]
list_csize= []
K= 4
for i in range(2,K+1,1):
  km= Kmeans(i,100,123)
  km.fit(X_std)
  centroids = km.centroids
  df_predicted= km.predict(X_std)
  arr = np.array(df_predicted)
  list1= []
  list2= []
  list3= []
  for j in range(0,i,1):
    count = np.count_nonzero(arr == j)
    list1.append(count)
 
  for j in range(0,len(df),1):
    
    list2.append(list1[df_predicted[j]])
    list3.append(centroids[df_predicted[j]])
  print(i)
  list_centroids.append(list3)
  list_csize.append(list2)

a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

with tf.device('/TPU:0'):
  c = tf.matmul(a, b)

print("c device: ", c.device)
print(c)

import tensorflow as tf

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
# This is the TPU initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print("All devices: ", tf.config.list_logical_devices('TPU'))

avgsimscore=[]
for i in range(0,len(df),1):
  upper=0
  lower=0
  for j in range(0,K-1,1):
    for l in range(j+1,K-1,1):
      lower= lower+ list_csize[j][i]+ list_csize[l][i]
      k= angle(list_centroids[j][i], list_centroids[l][i])
      if (k <= (3.14/2)):
        upper= upper+ (list_csize[j][i]+ list_csize[l][i])*(abs(math.cos(k)))
      else:
        upper= upper+ (list_csize[j][i]+ list_csize[l][i])*(abs(math.cos(3.14- k)))
  
  print(i)
  avgsimscore.append(upper/lower)

  print(avgsimscore[i])

avgsimscore

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
# if using a Jupyter notebook, include:
# %matplotlib inline
ones = []
for i in range(len(avgsimscore)):
    ones.append(1)
plt.figure(figsize=(15, 15), dpi=80)
plt.scatter(ones,avgsimscore)
plt.show()

import sklearn.metrics

auprc = sklearn.metrics.average_precision_score(array[:len(avgsimscore)], listans)

auprc

count=0
kkf = 0
listans=[0]*len(avgsimscore)
for i in range(0,len(avgsimscore),1):
  if(array[i]==1 and avgsimscore[i]<=0.6):
    kkf= kkf+1
  if(avgsimscore[i]<=0.6):
    count= count+1
    listans[i]=1

kkf

df.index[0]

accuracy = (count/kkf)*100

accuracy

count